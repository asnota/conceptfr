{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Finetuning"
      ],
      "metadata": {
        "id": "0WJ3E4saxez3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial set-up"
      ],
      "metadata": {
        "id": "0JaJZaIgoysH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installs"
      ],
      "metadata": {
        "id": "gcMOeJ1LpF4O"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IlQvPDxrSjb5"
      },
      "outputs": [],
      "source": [
        "%pip install codecarbon comet_ml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yXsd7ugYWsTs"
      },
      "outputs": [],
      "source": [
        "%pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Carbon emission tracker launch"
      ],
      "metadata": {
        "id": "0X6fDORapK33"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAEzAF_mSsss"
      },
      "outputs": [],
      "source": [
        "from comet_ml import Experiment\n",
        "from codecarbon import EmissionsTracker\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialise and start CodeCarbon tracker\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f'Start time is {start_time}')\n",
        "\n",
        "# Initialise the Comet experiment\n",
        "experiment = Experiment(\n",
        "    api_key=\"XXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
        "    project_name=\"\",\n",
        "    workspace=\"\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Drive mounting"
      ],
      "metadata": {
        "id": "KW5tutCOpPHU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iF984qgtht1"
      },
      "outputs": [],
      "source": [
        "# Mount gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_path = \"/content/gdrive/MyDrive/GEMFR/\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data"
      ],
      "metadata": {
        "id": "-R2tl7L2ppiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data load"
      ],
      "metadata": {
        "id": "FGjTyyTsqCV9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V92abZx6ttYw"
      },
      "outputs": [],
      "source": [
        "# Train set JSON\n",
        "\n",
        "import json\n",
        "\n",
        "f = open(gdrive_path + \"automatically_cleaned/\" + \"train_set_clean.json\")\n",
        "\n",
        "raw_train_data = json.load(f)\n",
        "len(raw_train_data)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set ConceptFR JSON\n",
        "\n",
        "f = open(gdrive_path + \"automatically_cleaned/\" + \"val_set.json\")\n",
        "raw_val_data = json.load(f)"
      ],
      "metadata": {
        "id": "Aw3vYC-W3MoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train set: change raw data to concept-target format\n",
        "\n",
        "en_entries_train = []\n",
        "fr_entries_train = []\n",
        "\n",
        "for entry in raw_train_data:\n",
        "    en_entry = {\n",
        "        \"concept\": entry[\"english_concepts\"],\n",
        "        \"target\": entry[\"english_example\"]\n",
        "    }\n",
        "    en_entries_train.append(en_entry)\n",
        "\n",
        "    fr_entry = {\n",
        "        \"concept\": entry[\"french_concepts\"],\n",
        "        \"target\": entry[\"french_example\"]\n",
        "    }\n",
        "    fr_entries_train.append(fr_entry)\n",
        "print(len(en_entries_train))\n",
        "print(len(fr_entries_train))"
      ],
      "metadata": {
        "id": "coMNvUOfEv_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Validation set: change raw data to concept-target format\n",
        "\n",
        "en_entries_val = []\n",
        "fr_entries_val = []\n",
        "\n",
        "for entry in raw_val_data:\n",
        "    en_entry = {\n",
        "        \"concept\": entry[\"english_concepts\"],\n",
        "        \"target\": entry[\"english_example\"]\n",
        "    }\n",
        "    en_entries_val.append(en_entry)\n",
        "\n",
        "    fr_entry = {\n",
        "        \"concept\": entry[\"french_concepts\"],\n",
        "        \"target\": entry[\"french_example\"]\n",
        "    }\n",
        "    fr_entries_val.append(fr_entry)\n",
        "print(len(en_entries_val))\n",
        "print(len(fr_entries_val))"
      ],
      "metadata": {
        "id": "g1C55SCsH8CI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conversion to a Dataset object"
      ],
      "metadata": {
        "id": "5wHtVWiIqG4M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NNNEheELudHg"
      },
      "outputs": [],
      "source": [
        "# Convert train and val sets to JSON Lines to feed Dataset class\n",
        "\n",
        "# ConceptFR French partition\n",
        "train_data = fr_entries_train\n",
        "val_data = fr_entries_val\n",
        "\n",
        "with open('json_lines_train.jl', 'w') as outfile:\n",
        "    for entry in train_data:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')\n",
        "\n",
        "with open('json_lines_val.jl', 'w') as outfile:\n",
        "    for entry in val_data:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create train and validation splits"
      ],
      "metadata": {
        "id": "wzSAppF5qOp9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ViEzoUUuzHU"
      },
      "outputs": [],
      "source": [
        "# Create a dataset objects\n",
        "from datasets import Dataset\n",
        "\n",
        "train_dataset = Dataset.from_json(\"json_lines_train.jl\")\n",
        "validation_dataset = Dataset.from_json(\"json_lines_val.jl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bOsfwnqwZmG"
      },
      "outputs": [],
      "source": [
        "# Put both splits to the DatasetDict object, the Accelerator understands\n",
        "from datasets import DatasetDict\n",
        "\n",
        "data = DatasetDict({\"train\":train_dataset,\"validation\": validation_dataset})"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model finetuning"
      ],
      "metadata": {
        "id": "xsa-FsHSqb5_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model parameters"
      ],
      "metadata": {
        "id": "kKV9qkKgqmmP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl5JRx_Q0TT9"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "MODEL_NAME = \"facebook/bart-base\"\n",
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "RANDOM_SEED = 42\n",
        "BEAM_SIZE = 4\n",
        "MAX_LENGTH = 32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "model = model.to(DEVICE)\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batch input construction"
      ],
      "metadata": {
        "id": "ynR40lVRq7fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wS5I3J9I0pOn"
      },
      "outputs": [],
      "source": [
        "# Construct input strings from a batch.\n",
        "\n",
        "def construct_input_for_batch(batch):\n",
        "    source = batch[\"concept\"] \n",
        "    target = batch[\"target\"]\n",
        "    return source, target\n",
        "\n",
        "# Construct the batch (source, target) and run them through a tokenizer.\n",
        "\n",
        "def batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH ):\n",
        "    source, target = construct_input_for_batch(batch)\n",
        "    res = {\n",
        "        \"input_ids\": tokenizer(source)[\"input_ids\"],\n",
        "        \"labels\": tokenizer(target, padding=\"max_length\", truncation=True, max_length=max_length)[\"input_ids\"],\n",
        "    }\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ItpVmiz1Ak-"
      },
      "outputs": [],
      "source": [
        "# Map the function to the training and validation sets\n",
        "\n",
        "train_data_tokenized = data['train'].map(\n",
        "    lambda batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\n",
        "    batched=True\n",
        ")\n",
        "valid_data_tokenized = data['validation'].map(\n",
        "    lambda batch: batch_tokenize(batch, tokenizer, max_length=MAX_LENGTH),\n",
        "    batched=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trainer set-up & run"
      ],
      "metadata": {
        "id": "yqcyYLJxrSc0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3siaxGDK2hA9"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "GRADIENT_ACCUMULATION_STEPS = 1\n",
        "LEARNING_RATE = 1e-04\n",
        "EPOCHS = 10\n",
        "WARMUP_STEPS = 1000\n",
        "\n",
        "# Define train args and pass params to the trainer\n",
        "train_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"BART-experiments\",\n",
        "    evaluation_strategy=\"epoch\", \n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=100,\n",
        "\n",
        "    # optimization args, the trainer uses the Adam optimizer\n",
        "    # and has a linear warmup for the learning rate\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    per_device_eval_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_epochs=EPOCHS,\n",
        "    warmup_steps=WARMUP_STEPS,\n",
        "\n",
        "    # misc args\n",
        "    seed=RANDOM_SEED,\n",
        "    disable_tqdm=False,\n",
        "    load_best_model_at_end=True,\n",
        "    \n",
        "    # generation\n",
        "    predict_with_generate=True,\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=train_args,\n",
        "    train_dataset=train_data_tokenized,\n",
        "    eval_dataset=valid_data_tokenized,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "trainer._max_length = MAX_LENGTH\n",
        "trainer._num_beams = BEAM_SIZE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fkWGN7Dh2m0f"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DL6UrCWH6z8A"
      },
      "source": [
        "### Save finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6w8PF8O62kC"
      },
      "outputs": [],
      "source": [
        "# Save the finetuned model\n",
        "\n",
        "torch.save(model, \"/content/MyModel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o4FX_I52BPfa"
      },
      "outputs": [],
      "source": [
        "# Save statedict of the finetuned model\n",
        "torch.save(model.state_dict(), \"/content/MyModelStateDict\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsXBMvRdCLoP"
      },
      "source": [
        "## Load finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJKNaNCOJ_70"
      },
      "outputs": [],
      "source": [
        "# Load the model if necessary\n",
        "new_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "new_model.load_state_dict((torch.load(\"/content/MyModelStateDict\"))) # Load state dict\n",
        "new_model.to(DEVICE)\n",
        "new_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "loaded_model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME)\n",
        "loaded_model = torch.load(\"/content/MyModel\")"
      ],
      "metadata": {
        "id": "W1Pg8Vwvdz7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XMFtr2tCIB4"
      },
      "source": [
        "## Generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text generation function (beam search decoding)"
      ],
      "metadata": {
        "id": "zOVbhG8-s6Ad"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVYmqbMxJ86a"
      },
      "outputs": [],
      "source": [
        "# Generating and evaluating predictions\n",
        "\n",
        "def beam_generate_sentences(\n",
        "    batch,\n",
        "    model,\n",
        "    tokenizer,\n",
        "    num_beams=4,\n",
        "    max_length=32,\n",
        "    device=\"cpu\"\n",
        "):\n",
        "    \"\"\"Generate outputs from a model with beam search decoding.\"\"\"\n",
        "    \n",
        "    # Create batch inputs (only concepts are needed here).\n",
        "    source, _ = construct_input_for_batch(batch)\n",
        "\n",
        "    # Use the model's tokenizer to create the batch input_ids.\n",
        "    batch_features = tokenizer(source, padding=True, return_tensors='pt')\n",
        "\n",
        "    # Move all inputs to the device.\n",
        "    batch_features = dict([(k, v.to(device)) for k, v in batch_features.items()])\n",
        "\n",
        "    # Generate with beam search.\n",
        "    generated_ids = model.generate(\n",
        "        **batch_features,\n",
        "        num_beams=num_beams,\n",
        "        max_length=max_length,\n",
        "    )\n",
        "\n",
        "    # Use model tokenizer to decode to text.\n",
        "    generated_sentences = [\n",
        "        tokenizer.decode(gen_ids.tolist(), skip_special_tokens=True)\n",
        "        for gen_ids in generated_ids\n",
        "    ]\n",
        "    return generated_sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Generate text with concepts from French and English test set of the ConceptFR dataset"
      ],
      "metadata": {
        "id": "vQYxIbxJt5Y2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Open a JSON with selected entries from ConceptFR FR val \n",
        "\n",
        "f = open(gdrive_path + \"automatically_cleaned/\" + \"test_set.json\")\n",
        "raw_test_data = json.load(f)"
      ],
      "metadata": {
        "id": "FeXh7OoNYSxR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test set: change raw data to concept-target format\n",
        "\n",
        "en_entries_test = []\n",
        "fr_entries_test = []\n",
        "\n",
        "for entry in raw_test_data:\n",
        "    en_entry = {\n",
        "        \"concept\": entry[\"english_concepts\"],\n",
        "        \"target\": entry[\"english_example\"]\n",
        "    }\n",
        "    en_entries_test.append(en_entry)\n",
        "\n",
        "    fr_entry = {\n",
        "        \"concept\": entry[\"french_concepts\"],\n",
        "        \"target\": entry[\"french_example\"]\n",
        "    }\n",
        "    fr_entries_test.append(fr_entry)\n",
        "print(len(en_entries_test))\n",
        "print(len(fr_entries_test))"
      ],
      "metadata": {
        "id": "ji39ouIlYkqD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to JSON Lines to feed Dataset class\n",
        "\n",
        "# English partition of ConceptFR test set\n",
        "test_data = en_entries_test\n",
        "\n",
        "# French partition of ConceptFR test set\n",
        "#test_data = fr_entries_test\n",
        "\n",
        "with open('json_lines_test.jl', 'w') as outfile:\n",
        "    for entry in test_data:\n",
        "        json.dump(entry, outfile)\n",
        "        outfile.write('\\n')"
      ],
      "metadata": {
        "id": "8R3Wb0zXZTVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dataset object\n",
        "\n",
        "test_set = Dataset.from_json(\"json_lines_test.jl\")"
      ],
      "metadata": {
        "id": "boevamMQV9Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_output = test_set.map(\n",
        "    lambda batch: {'generated': beam_generate_sentences(\n",
        "        batch,\n",
        "        model,\n",
        "        tokenizer,\n",
        "        num_beams=BEAM_SIZE,\n",
        "        max_length=MAX_LENGTH,\n",
        "        device=DEVICE)\n",
        "    },\n",
        "    batched=True,\n",
        "    batch_size=128,\n",
        ")"
      ],
      "metadata": {
        "id": "iq8_GKIgWRN2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert generated entries to a list object"
      ],
      "metadata": {
        "id": "3BZpZT2vu9Wt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ1MSn82azQ8"
      },
      "outputs": [],
      "source": [
        "# Convert to list in order to save, as Dataset is not JSON serializable\n",
        "\n",
        "test_set_with_generated_entries = []\n",
        "for i, output in enumerate(valid_output):\n",
        "    json_object = {\n",
        "        \"concept\": output[\"concept\"],\n",
        "        \"target\": output[\"target\"],\n",
        "        \"generated\": output[\"generated\"]\n",
        "\n",
        "    }\n",
        "    test_set_with_generated_entries.append(json_object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzZQ08WxQOJS"
      },
      "source": [
        "### Save generated text "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaA68ETcQRpz"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_name = \"2nd_Experiment_cgen-conceptfr-en\"\n",
        "\n",
        "\n",
        "with open(gdrive_path + \"generated_text/\" + file_name + \".json\", 'w') as outfile:\n",
        "    json.dump(test_set_with_generated_entries, outfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Close the experiment after finetuning"
      ],
      "metadata": {
        "id": "7uVCXnRPxovJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stop CO2 tracker and print emissions\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"Emissions: {emissions} kg\")\n",
        "\n",
        "# Calculate the time spent\n",
        "stop_time = datetime.now() - start_time\n",
        "\n",
        "# Log the time to Comet\n",
        "hyper_params = {\n",
        "    \"time spent\": stop_time,\n",
        "    \"emmissions\": emissions,\n",
        "    \"batch size\": BATCH_SIZE,\n",
        "    \"gradient accumulation steps\": GRADIENT_ACCUMULATION_STEPS,\n",
        "    \"learning rate\": LEARNING_RATE,\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"warmup steps\": WARMUP_STEPS\n",
        "}\n",
        "\n",
        "# Hyperparameters\n",
        "experiment.log_parameters(hyper_params)\n",
        "\n",
        "# Turn off Comet\n",
        "experiment.end()"
      ],
      "metadata": {
        "id": "6kxxVpWJxugw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Metrics"
      ],
      "metadata": {
        "id": "HA50sGIZyqdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reopen the experiment for metrics logging"
      ],
      "metadata": {
        "id": "WkE7LVKuyEE0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from comet_ml import Experiment\n",
        "from codecarbon import EmissionsTracker\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialise and start CodeCarbon tracker\n",
        "tracker = EmissionsTracker()\n",
        "tracker.start()\n",
        "\n",
        "start_time = datetime.now()\n",
        "print(f'Start time is {start_time}')\n",
        "\n",
        "# Initialise the Comet experiment\n",
        "experiment = Experiment(\n",
        "    api_key=\"XXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
        "    project_name=\"\",\n",
        "    workspace=\"\",\n",
        ")"
      ],
      "metadata": {
        "id": "O2LKk7A7yC62"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gufV3hk4BIAl"
      },
      "source": [
        "## Open generated text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hRt613BpBLP6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "file_name = \"2nd_Experiment_cgen-conceptfr-fr\"\n",
        "\n",
        "f = open(gdrive_path + \"generated_text/\" + file_name + \".json\")\n",
        "valid_output = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E0wap4uxtWoN"
      },
      "source": [
        "## Calculate metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iw3ahqftcYC"
      },
      "outputs": [],
      "source": [
        "%pip install 'gem-metrics[heavy] @ git+https://github.com/GEM-benchmark/GEM-metrics.git'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjA70Yw5vGZ9"
      },
      "source": [
        "### Lexical metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45kUhFs-u2dG"
      },
      "outputs": [],
      "source": [
        "# Apply in a single pass format\n",
        "import gem_metrics\n",
        "\n",
        "list_of_predictions = [valid_output[0][\"generated\"]]\n",
        "list_of_references = [valid_output[0][\"target\"]]\n",
        "\n",
        "preds = gem_metrics.texts.Predictions(list_of_predictions)\n",
        "refs = gem_metrics.texts.References(list_of_references)\n",
        "\n",
        "result = gem_metrics.compute(preds, refs, metrics_list=['bleu', 'rouge', 'nist', 'meteor'])\n",
        "print(list_of_predictions, \"|\", list_of_references)\n",
        "result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aM3Sk3vvwJ57"
      },
      "outputs": [],
      "source": [
        "# Calculate metrics in a loop\n",
        "\n",
        "metrics_result = []\n",
        "for i, output in enumerate(valid_output):\n",
        "    print(i)\n",
        "    prediction = [output[\"generated\"]]\n",
        "    reference = [output[\"target\"]]\n",
        "\n",
        "    preds = gem_metrics.texts.Predictions(prediction)\n",
        "    refs = gem_metrics.texts.References(reference)\n",
        "\n",
        "    result_lexical = gem_metrics.compute(preds, refs, metrics_list=['bleu', 'rouge', 'nist', 'meteor'])\n",
        "    metrics_result.append(result_lexical)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bOqGejmQvFbF"
      },
      "outputs": [],
      "source": [
        "# Extract values\n",
        "bleu = []\n",
        "rouge1_precision = []\n",
        "rouge1_recall = []\n",
        "rouge1_fmeasure = []\n",
        "rouge2_precision = []\n",
        "rouge2_recall = []\n",
        "rouge2_fmeasure = []\n",
        "rougeL_precision = []\n",
        "rougeL_recall = []\n",
        "rougeL_fmeasure = []\n",
        "nist = []\n",
        "meteor = []\n",
        "\n",
        "for metric_object in metrics_result:\n",
        "    bleu.append(metric_object.get(\"bleu\"))\n",
        "\n",
        "    rouge1_precision.append(metric_object[\"rouge1\"].get(\"precision\"))\n",
        "    rouge1_recall.append(metric_object[\"rouge1\"].get(\"recall\"))\n",
        "    rouge1_fmeasure.append(metric_object[\"rouge1\"].get(\"fmeasure\"))\n",
        "\n",
        "    rouge2_precision.append(metric_object[\"rouge2\"].get(\"precision\"))\n",
        "    rouge2_recall.append(metric_object[\"rouge2\"].get(\"recall\"))\n",
        "    rouge2_fmeasure.append(metric_object[\"rouge2\"].get(\"fmeasure\"))\n",
        "\n",
        "    rougeL_precision.append(metric_object[\"rougeL\"].get(\"precision\"))\n",
        "    rougeL_recall.append(metric_object[\"rougeL\"].get(\"recall\"))\n",
        "    rougeL_fmeasure.append(metric_object[\"rougeL\"].get(\"fmeasure\"))\n",
        "\n",
        "    nist.append(metric_object.get(\"nist\"))\n",
        "    meteor.append(metric_object.get(\"meteor\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vK-GIFoZ38tK"
      },
      "outputs": [],
      "source": [
        "# Calculate mean, min and max values for each metric\n",
        "\n",
        "bleu_average = sum(bleu)/len(bleu)\n",
        "\n",
        "rouge1_precision_average = sum(rouge1_precision)/len(rouge1_precision)\n",
        "rouge1_recall_average = sum(rouge1_recall)/len(rouge1_recall)\n",
        "rouge1_fmeasure_average = sum(rouge1_fmeasure)/len(rouge1_fmeasure)\n",
        "\n",
        "rouge2_precision_average = sum(rouge2_precision)/len(rouge2_precision)\n",
        "rouge2_recall_average = sum(rouge2_recall)/len(rouge2_recall)\n",
        "rouge2_fmeasure_average = sum(rouge2_fmeasure)/len(rouge2_fmeasure)\n",
        "\n",
        "rougeL_precision_average = sum(rougeL_precision)/len(rougeL_precision)\n",
        "rougeL_recall_average = sum(rougeL_recall)/len(rougeL_recall)\n",
        "rougeL_fmeasure_average = sum(rougeL_fmeasure)/len(rougeL_fmeasure)\n",
        "\n",
        "nist_average = sum(nist)/len(nist)\n",
        "meteor_average = sum(meteor)/len(meteor)\n",
        "\n",
        "print(\"bleu_average\", bleu_average)\n",
        "print(\"rouge1_precision_average\", rouge1_precision_average)\n",
        "print(\"rouge1_recall_average\", rouge1_recall_average)\n",
        "print(\"rouge1_fmeasure_average\", rouge1_fmeasure_average)\n",
        "print(\"rouge2_precision_average\", rouge2_precision_average)\n",
        "print(\"rouge2_recall_average\", rouge2_recall_average)\n",
        "print(\"rouge2_fmeasure_average\", rouge2_fmeasure_average)\n",
        "print(\"rougeL_precision_average\", rougeL_precision_average)\n",
        "print(\"rougeL_recall_average\", rougeL_recall_average)\n",
        "print(\"rougeL_fmeasure_average\", rougeL_fmeasure_average)\n",
        "print(\"nist_average\", nist_average)\n",
        "print(\"meteor_average\", meteor_average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aT61S30uDu-N"
      },
      "outputs": [],
      "source": [
        "# Extract preds and refs from a list object\n",
        "\n",
        "preds = []\n",
        "refs = []\n",
        "for output in valid_output:\n",
        "    preds.append(output[\"generated\"])\n",
        "    refs.append(output[\"target\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zi46bCT4ZmoO"
      },
      "outputs": [],
      "source": [
        "# Calculate MSTTR\n",
        "\n",
        "predictions = gem_metrics.texts.Predictions(preds)\n",
        "references = gem_metrics.texts.References(refs)\n",
        "\n",
        "result_msttr = gem_metrics.compute(predictions, metrics_list=['msttr'])\n",
        "print(result_msttr)\n",
        "\n",
        "msttr_100 = result_msttr[\"msttr-100\"]\n",
        "print(msttr_100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l5oj6Kt6gSrC"
      },
      "outputs": [],
      "source": [
        "# Calculate ngrams without reference\n",
        "import gem_metrics\n",
        "\n",
        "result_ngrams = gem_metrics.compute(predictions, metrics_list=['ngrams'])\n",
        "result_ngrams"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckJ_KwpvS3Vc"
      },
      "source": [
        "### Semantic metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRSXwobkVCmr"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/google-research/bleurt.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrv_CriSc6Sm"
      },
      "outputs": [],
      "source": [
        "%pip install git+https://github.com/Tiiiger/bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Czat7pR4YUpN"
      },
      "outputs": [],
      "source": [
        "# check installation\n",
        "import bert_score\n",
        "bert_score.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnqpYC9mYbjv"
      },
      "outputs": [],
      "source": [
        "# Native bertscore\n",
        "from bert_score import score\n",
        "\n",
        "bert_precision, bert_recall, bert_f1 = score(preds, refs, lang='en', verbose=True)\n",
        "print(bert_precision, bert_recall, bert_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwjmzD7hKGjm"
      },
      "outputs": [],
      "source": [
        "# Calculate BERTscore average\n",
        "\n",
        "bertscore_precision_average = bert_precision.mean() \n",
        "bertscore_recall_average = bert_recall.mean()\n",
        "bertscore_f1_average = bert_f1.mean()\n",
        "print(\"bertscore_precision_average\", bertscore_precision_average)\n",
        "print(\"bertscore_recall_average\", bertscore_recall_average)\n",
        "print(\"bertscore_f1_average\", bertscore_f1_average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfmP-TbaNcMp"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "\n",
        "bleurt_metric = load_metric(\"bleurt\")\n",
        "bleurt = []\n",
        "\n",
        "#bertscore_metric = load_metric(\"bertscore\")\n",
        "#bertscore = []\n",
        "\n",
        "for i, output in enumerate(valid_output):\n",
        "    #bertscore_result = bertscore_metric.compute(predictions=[output[\"generated\"]], references=[output[\"target\"]], lang=\"bert-base-multilingual-cased\")\n",
        "    #bertscore.append(bertscore_result)\n",
        "  \n",
        "    bleurt_result = bleurt_metric.compute(predictions=[output[\"generated\"]], references=[output[\"target\"]])\n",
        "    bleurt.append(bleurt_result[\"scores\"][0])\n",
        "      \n",
        "    print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xa4n5kw0PLW7"
      },
      "outputs": [],
      "source": [
        "# Calculate BLEURT average\n",
        "\n",
        "bleurt_average = sum(bleurt)/len(bleurt)\n",
        "print(\"bleurt_average\", bleurt_average)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zwUf_2b5Rujx"
      },
      "outputs": [],
      "source": [
        "# Calculate repetitions\n",
        "\n",
        "same = []\n",
        "for output in valid_output:\n",
        "    if output[\"generated\"] == output[\"target\"]:\n",
        "        same.append(output)\n",
        "print(\"number of identical to validation\", len(same))\n",
        "\n",
        "repetition_percentage = (len(same) / len(valid_output)) * 100\n",
        "print(\"repetition_percentage\", repetition_percentage, \"%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjBgdJiwDqFL"
      },
      "outputs": [],
      "source": [
        "def count_concepts_coverage(concepts, phrase):\n",
        "    covered_concepts = []\n",
        "    for concept in concepts:\n",
        "        for word in phrase:\n",
        "            if word.startswith(concept[:3]):\n",
        "                covered_concepts.append(word)\n",
        "    not_covered_len = len(concepts) - len(set(covered_concepts))\n",
        "    return not_covered_len, set(covered_concepts)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Concepts coverage in generated text"
      ],
      "metadata": {
        "id": "0u1ChHW26nXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mXel4MYRxaX"
      },
      "outputs": [],
      "source": [
        "# Find concepts coverage\n",
        "\n",
        "concepts_covered = []\n",
        "anomalies = []\n",
        "for i, output in enumerate(valid_output):\n",
        "    concepts = output[\"concept\"].split(\" \")\n",
        "    phrase = output[\"generated\"].split(\" \")\n",
        "    not_covered_len, covered_concepts = count_concepts_coverage(concepts, phrase)\n",
        "    non_coverage_percentage = ((not_covered_len / len(concepts)) * 100)\n",
        "    if non_coverage_percentage < 0:\n",
        "        anomalies.append(i)\n",
        "        non_coverage_percentage = 0\n",
        "    concepts_covered_object = {\n",
        "        \"concepts\": concepts,\n",
        "        \"generated_phrase\": phrase,\n",
        "        \"covered_concepts\": covered_concepts,\n",
        "        \"concepts_len\": len(concepts),\n",
        "        \"not_covered_len\": not_covered_len,\n",
        "        \"non_coverage_percentage\": non_coverage_percentage\n",
        "    }\n",
        "    concepts_covered.append(concepts_covered_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23kylbZEmpdH"
      },
      "outputs": [],
      "source": [
        "len(anomalies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_tXW-gpGFDg"
      },
      "outputs": [],
      "source": [
        "# Calculate the percentage of the concepts coverage\n",
        "\n",
        "non_coverage_percentage_list = []\n",
        "for concepts_covered_object in concepts_covered:\n",
        "    non_coverage_percentage_list.append(concepts_covered_object[\"non_coverage_percentage\"])\n",
        "non_coverage_average = sum(non_coverage_percentage_list) / len(non_coverage_percentage_list)\n",
        "print(\"non_coverage_average\", non_coverage_average)\n",
        "\n",
        "coverage_percentage = 100 - non_coverage_average\n",
        "print(\"coverage_percentage\", coverage_percentage)\n",
        "\n",
        "minimum_non_covered_value = min(non_coverage_percentage_list)\n",
        "print(\"minimum_non_covered_value\", minimum_non_covered_value)\n",
        "\n",
        "maximum_non_covered_value = max(non_coverage_percentage_list)\n",
        "print(\"maximum_non_covered_value\", maximum_non_covered_value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h1tXumpTF4j"
      },
      "source": [
        "## Stop CO2 tracker and Comet session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "utd8-4kCTE8K"
      },
      "outputs": [],
      "source": [
        "# Stop CO2 tracker and print emissions\n",
        "\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"Emissions: {emissions} kg\")\n",
        "\n",
        "# Calculate the time spent\n",
        "stop_time = datetime.now() - start_time\n",
        "\n",
        "# Lexical metrics\n",
        "experiment.log_metric(\"bleu_average\", bleu_average)\n",
        "experiment.log_metric(\"rouge1_precision_average\", rouge1_precision_average)\n",
        "experiment.log_metric(\"rouge1_recall_average\", rouge1_recall_average)\n",
        "experiment.log_metric(\"rouge1_fmeasure_average\", rouge1_fmeasure_average)\n",
        "experiment.log_metric(\"rouge2_precision_average\", rouge2_precision_average)\n",
        "experiment.log_metric(\"rouge2_recall_average\", rouge2_recall_average)\n",
        "experiment.log_metric(\"rouge2_fmeasure_average\", rouge2_fmeasure_average)\n",
        "experiment.log_metric(\"rougeL_precision_average\", rougeL_precision_average)\n",
        "experiment.log_metric(\"rougeL_recall_average\", rougeL_recall_average)\n",
        "experiment.log_metric(\"rougeL_fmeasure_average\", rougeL_fmeasure_average)\n",
        "experiment.log_metric(\"nist_average\", nist_average)\n",
        "experiment.log_metric(\"meteor_average\", meteor_average)\n",
        "\n",
        "# Sematic metrics\n",
        "experiment.log_metric(\"bertscore_precision_average\", bertscore_precision_average)\n",
        "experiment.log_metric(\"bertscore_recall_average\", bertscore_recall_average)\n",
        "experiment.log_metric(\"bertscore_f1_average\", bertscore_f1_average)\n",
        "experiment.log_metric(\"bleurt_average\", bleurt_average)\n",
        "\n",
        "# GEM diversity metrics\n",
        "experiment.log_metric(\"msttr_100\", msttr_100)\n",
        "experiment.log_metric(\"distict_1_gem\", result_ngrams[\"distinct-1\"])\n",
        "experiment.log_metric(\"distict_2_gem\", result_ngrams[\"distinct-2\"])\n",
        "experiment.log_metric(\"distict_3_gem\", result_ngrams[\"distinct-3\"])\n",
        "experiment.log_metric(\"unique_1_gem\", result_ngrams[\"unique-1\"])\n",
        "experiment.log_metric(\"unique_2_gem\", result_ngrams[\"unique-2\"])\n",
        "experiment.log_metric(\"unique_3_gem\", result_ngrams[\"unique-3\"])\n",
        "experiment.log_metric(\"entropy_1_gem\", result_ngrams[\"entropy-1\"])\n",
        "experiment.log_metric(\"entropy_2_gem\", result_ngrams[\"entropy-2\"])\n",
        "experiment.log_metric(\"entropy_3_gem\", result_ngrams[\"entropy-3\"])\n",
        "experiment.log_metric(\"cond_entropy_2_gem\", result_ngrams[\"cond_entropy-2\"])\n",
        "experiment.log_metric(\"cond_entropy_3_gem\", result_ngrams[\"cond_entropy-3\"])\n",
        "experiment.log_metric(\"vocab_size_1_gem\", result_ngrams[\"vocab_size-1\"])\n",
        "experiment.log_metric(\"vocab_size_2_gem\", result_ngrams[\"vocab_size-2\"])\n",
        "experiment.log_metric(\"vocab_size_3_gem\", result_ngrams[\"vocab_size-3\"])\n",
        "experiment.log_metric(\"min_pred_length\", result_ngrams[\"min_pred_length\"])\n",
        "experiment.log_metric(\"max_pred_length\", result_ngrams[\"max_pred_length\"])\n",
        "\n",
        "# Coverage and repetitions\n",
        "experiment.log_metric(\"coverage_percentage\", coverage_percentage) \n",
        "experiment.log_metric(\"repetition_percentage\", repetition_percentage)\n",
        "\n",
        "# Turn off Comet\n",
        "experiment.end()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}