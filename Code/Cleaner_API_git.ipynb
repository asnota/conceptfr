{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Basic API"
      ],
      "metadata": {
        "id": "JbrL16ka01f-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob3Je2oemESG"
      },
      "outputs": [],
      "source": [
        "!pip install langdetect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q31-7V6j9lpr"
      },
      "outputs": [],
      "source": [
        "!pip install datasets bert_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PEg1Rfojnvpa"
      },
      "outputs": [],
      "source": [
        "# Basic API\n",
        "import json\n",
        "import os\n",
        "from langdetect import detect\n",
        "from datasets import load_metric\n",
        "\n",
        "bertscore = load_metric(\"bertscore\")\n",
        "\n",
        "def load_doc(doc):\n",
        "    f = open(doc)\n",
        "    entries = json.load(f)\n",
        "    return entries\n",
        "\n",
        "def load_dir(path, gdrive_path, dir):\n",
        "    all = []\n",
        "    flatten = []\n",
        "    for file in path:\n",
        "        print(file)\n",
        "        doc = load_doc(gdrive_path + dir + file)\n",
        "        all.append(doc)\n",
        "    flatten = [entry for doc in all for entry in doc]\n",
        "    return flatten\n",
        "\n",
        "def extract_arrs(entries):\n",
        "    en_concepts = []\n",
        "    en_examples = []\n",
        "    fr_concepts = []\n",
        "    fr_examples = []\n",
        "    fr_tags = []\n",
        "    for i in range(len(entries)):\n",
        "        en_concepts.append(entries[i][\"english_concept\"])\n",
        "        en_examples.append(entries[i][\"english_example\"])\n",
        "        fr_concepts.append(entries[i][\"french_concept\"])\n",
        "        fr_examples.append(entries[i][\"french_example\"])\n",
        "        fr_tags.append(entries[i][\"french_concept_tag\"])\n",
        "    return en_concepts, fr_concepts, fr_tags, en_examples, fr_examples\n",
        "\n",
        "def lang_switcher(en_examples, fr_examples):\n",
        "    new_examples = []\n",
        "    miscellaneous_examples = []\n",
        "    for i, (example_en, example_fr) in enumerate(zip(en_examples, fr_examples)):\n",
        "        lang_en = detect(example_en)\n",
        "        lang_fr = detect(example_fr)\n",
        "\n",
        "        if lang_en == \"en\":\n",
        "            new_example = {\n",
        "                \"id\": i,\n",
        "                \"english_example\": example_en,\n",
        "\t\t        \"french_example\": example_fr\t\n",
        "            }\n",
        "            new_examples.append(new_example)\n",
        "\n",
        "        elif lang_fr == \"fr\":\n",
        "            new_example = {\n",
        "                \"id\": i,\n",
        "                \"english_example\": example_en,\n",
        "\t\t        \"french_example\": example_fr\t\n",
        "            }\n",
        "            new_examples.append(new_example)\n",
        "\n",
        "        elif lang_en == \"fr\":\n",
        "            new_example = {\n",
        "                \"id\": i,\n",
        "                \"english_example\": example_fr,\n",
        "\t\t        \"french_example\": example_en\t\n",
        "            }\n",
        "            new_examples.append(new_example)\n",
        "\n",
        "        elif lang_fr == \"en\":\n",
        "            new_example = {\n",
        "                \"id\": i,\n",
        "                \"english_example\": example_fr,\n",
        "\t\t        \"french_example\": example_en\t\n",
        "            }\n",
        "            new_examples.append(new_example)\n",
        "\n",
        "        else:\n",
        "            miscellaneous_example = {\n",
        "                \"id\": i,\n",
        "                \"english_example\": example_en,\n",
        "\t\t        \"french_example\": example_fr\n",
        "            },\n",
        "\n",
        "            new_example = {\n",
        "                \"id\": i,\n",
        "                \"english_example\": example_en,\n",
        "\t\t        \"french_example\": example_fr\n",
        "            }\n",
        "\n",
        "            miscellaneous_examples.append(miscellaneous_example)\n",
        "            new_examples.append(new_example)\n",
        "\n",
        "    return new_examples, miscellaneous_examples\n",
        "\n",
        "def save_corrected(en_concepts, fr_concepts, french_concept_tags, new_examples):\n",
        "    dataset_objets = []\n",
        "\n",
        "    for english_concept, french_concept, french_concept_tag, en_fr_example in zip(en_concepts, fr_concepts, french_concept_tags, new_examples):\n",
        "        dataset_object = {\n",
        "                \"id\": en_fr_example.get(\"id\"),\n",
        "                \"english_concept\": english_concept,\n",
        "                \"french_concept\": french_concept,       \n",
        "                \"french_concept_tag\": french_concept_tag,\n",
        "                \"english_example\": en_fr_example.get(\"english_example\"),\n",
        "                \"french_example\": en_fr_example.get(\"french_example\")\n",
        "            }\n",
        "        dataset_objets.append(dataset_object)\n",
        "    file_name = 'corrected_lang_' + str(len(dataset_objets)) + '.json'\n",
        "    with open(gdrive_path + \"corrected_2/\" + file_name, 'w') as outfile:\n",
        "        json.dump(dataset_objets, outfile)\n",
        "    \n",
        "    return dataset_objets\n",
        "\n",
        "\n",
        "def extract_faulty(doc):\n",
        "    faulty_entries = []\n",
        "    valid_entries = []\n",
        "    for i, entry in enumerate(doc):\n",
        "        if entry.get(\"english_example\").startswith(\"ⓘCette phrase n'est pas une traduction de la phrase originale. \"):\n",
        "            faulty_entries.append(entry)\n",
        "        elif entry.get(\"french_example\").startswith(\"ⓘCette phrase n'est pas une traduction de la phrase originale. \"):\n",
        "            faulty_entries.append(entry)\n",
        "        else:\n",
        "            valid_entries.append(entry)\n",
        "            \n",
        "    return faulty_entries, valid_entries\n",
        "\n",
        "def retrieve_mismatched(doc):\n",
        "    matched_entries = []\n",
        "    mismatched_entries = []\n",
        "    for i, entry in enumerate(doc):\n",
        "        results = bertscore.compute(predictions=[entry.get(\"french_example\")], references=[entry.get(\"english_example\")], lang=\"bert-base-multilingual-cased\")\n",
        "        value = results.get(\"f1\")\n",
        "        if value[0] < 85:\n",
        "            mismatched_entries.append(entry)\n",
        "        else:\n",
        "            matched_entries.append(entry)\n",
        "    return mismatched_entries, matched_entries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QnuD96DZmHos",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88da8bf0-26b4-4e56-e72e-ee155b27b620"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount gdrive\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "gdrive_path = \"/content/gdrive/MyDrive/GEMFR/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_URG0_e6nd80"
      },
      "outputs": [],
      "source": [
        "# Remove automatically created checkpoints that won't be read by json\n",
        "!rm -rf `find -type d -name .ipynb_checkpoints`\n",
        "\n",
        "# Make sure the removal was effective\n",
        "print(os.listdir('gdrive/MyDrive/GEMFR'))\n",
        "print(os.listdir('gdrive/MyDrive/GEMFR/concepts'))\n",
        "print(os.listdir('gdrive/MyDrive/GEMFR/filtered'))\n",
        "print(os.listdir('gdrive/MyDrive/GEMFR/scraped'))\n",
        "\n",
        "# Get and extract dirs content\n",
        "concepts_path_content = os.listdir('gdrive/MyDrive/GEMFR/concepts')\n",
        "en_concepts_path_content = os.listdir('gdrive/MyDrive/GEMFR/en_concepts')\n",
        "filtered_path_content = os.listdir('gdrive/MyDrive/GEMFR/filtered')\n",
        "scraped_path_content = os.listdir('gdrive/MyDrive/GEMFR/scraped')\n",
        "corrected_path_content = os.listdir('gdrive/MyDrive/GEMFR/corrected')\n",
        "\n",
        "concepts_doc = load_dir(concepts_path_content, gdrive_path, \"concepts/\")\n",
        "en_concepts_doc = load_dir(en_concepts_path_content, gdrive_path, \"en_concepts/\")\n",
        "filtered_doc = load_dir(filtered_path_content, gdrive_path, \"filtered/\")\n",
        "scraped_doc = load_dir(scraped_path_content, gdrive_path, \"scraped/\")\n",
        "corrected_doc = load_dir(corrected_path_content, gdrive_path, \"corrected/\")\n",
        "ref_doc = load_doc(gdrive_path + 'unique_entries_str_14.json')\n",
        "\n",
        "# Extract concepts and phrases\n",
        "en_concepts_filtered, fr_concepts_filtered, french_concept_tags_filtered, en_examples_filtered, fr_examples_filtered = extract_arrs(filtered_doc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q29TSYjeykzn"
      },
      "outputs": [],
      "source": [
        "# Correct language switch\n",
        "new_examples, miscellaneous = lang_switcher(en_examples_filtered, fr_examples_filtered)\n",
        "\n",
        "# Save corrected\n",
        "dataset_objs = save_corrected(en_concepts_filtered, fr_concepts_filtered, french_concept_tags_filtered, new_examples)\n",
        "\n",
        "# Etract faulty entries where phrases begin with \"ⓘCette phrase n'est pas une traduction de la phrase originale. \"\n",
        "faulty_entries, valid_entries = extract_faulty(dataset_objs)\n",
        "\n",
        "# Get rid of too short concepts, which are faulty anyway\n",
        "en_concepts, fr_concepts, french_concept_tags, en_examples, fr_examples = extract_arrs(valid_entries)\n",
        "filtered_concepts = [concept for concept in en_concepts if len(concept) > 2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfLF9hEoZy5K"
      },
      "outputs": [],
      "source": [
        "# Save extractions\n",
        "file_name = 'faulty_entries_' + str(len(faulty_entries)) + '.json'\n",
        "with open(gdrive_path + \"corrected_2/\" + file_name, 'w') as outfile:\n",
        "    json.dump(faulty_entries, outfile)\n",
        "file_name = 'valid_entries_' + str(len(valid_entries)) + '.json'\n",
        "with open(gdrive_path + \"corrected_2/\" + file_name, 'w') as outfile:\n",
        "    json.dump(valid_entries, outfile)\n",
        "file_name = 'filtered_concepts_' + str(len(filtered_concepts)) + '.json'\n",
        "with open(gdrive_path + \"corrected_2/\" + file_name, 'w') as outfile:\n",
        "    json.dump(filtered_concepts, outfile)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve_mismatched(doc):\n",
        "    matched_entries = []\n",
        "    mismatched_entries = []\n",
        "    for i, entry in enumerate(doc):\n",
        "        results = bertscore.compute(predictions=[entry.get(\"french_example\")], references=[entry.get(\"english_example\")], lang=\"bert-base-multilingual-cased\")\n",
        "        value = results.get(\"f1\")\n",
        "        print(value[0])\n",
        "        print(entry)\n",
        "        if value[0] >= 0.70:\n",
        "            matched_entries.append(entry)\n",
        "        else:\n",
        "            mismatched_entries.append(entry)\n",
        "    return mismatched_entries, matched_entries\n",
        "\n",
        "mismatched_entries, matched_entries = retrieve_mismatched(valid_entries)"
      ],
      "metadata": {
        "id": "lLPy_7O1MCqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'mismatched_entries_' + str(len(mismatched_entries)) + '.json'\n",
        "with open(gdrive_path + \"corrected_2/\" + file_name, 'w') as outfile:\n",
        "    json.dump(mismatched_entries, outfile)\n",
        "file_name = 'matched_entries_' + str(len(matched_entries)) + '.json'\n",
        "with open(gdrive_path + \"corrected_2/\" + file_name, 'w') as outfile:\n",
        "    json.dump(matched_entries, outfile)"
      ],
      "metadata": {
        "id": "urTIaPJDdK0n"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pwrLUKqm1DNn",
        "9siPW4rx_dVA",
        "K8ALpJASWrKi",
        "kYZ0RbG_6J3J",
        "wBgBuNZhmBjy"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}